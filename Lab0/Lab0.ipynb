{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFbUpH8elWQ7"
   },
   "source": [
    "# **Part 1: Run MobileNet on GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoNr0MWd5e5m"
   },
   "source": [
    "In this tutorial, we will explore how to train a neural network with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoBtxdvR5lwM"
   },
   "source": [
    "### Setup (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oLGv2RjLYh2"
   },
   "source": [
    "We will first install a few packages that will be used in this tutorial and also define the path of CUDA library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3r7Sl2cG7nZF"
   },
   "outputs": [],
   "source": [
    "!pip install torchprofile 1>/dev/null\n",
    "!ldconfig /usr/lib64-nvidia 2>/dev/null\n",
    "!pip install onnx 1>/dev/null\n",
    "!pip install onnxruntime 1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYgp0au_LeAd"
   },
   "source": [
    "We will then import a few libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3uAhaCSlFrK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.optim import *\n",
    "from torch.optim.lr_scheduler import *\n",
    "from torch.utils.data import DataLoader\n",
    "from torchprofile import profile_macs\n",
    "from torchvision.datasets import *\n",
    "from torchvision.transforms import *\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-nNU83gqm9U",
    "outputId": "d8d2ec35-ce3b-4329-91cc-2c3d79eae7e4"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1Yx0rDUK5fx"
   },
   "source": [
    "To ensure the reproducibility, we will control the seed of random generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_l1wEdeHOlu",
    "outputId": "58ebcd32-88de-4c78-b326-1c5e51b6c0e1"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzMCWN0aJNvl"
   },
   "source": [
    "We must decide the HYPER-parameter before training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyLHUYWZJNCJ"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "# TODO:\n",
    "# Decide your own hyper-parameters\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCH = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Y0sLyajGAu"
   },
   "source": [
    "### Data  (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAbL_li0KPsz"
   },
   "source": [
    "In this lab, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of\n",
    "size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PO2mP2GytNl"
   },
   "source": [
    "Before using the data as input, we can do data pre-processing with transform function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pqhy8EJSjJfp",
    "outputId": "923bf6fc-a444-4082-dd87-3349f7b755be"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Resize images to 224x224, i.e., the input image size of MobileNet,\n",
    "# Convert images to PyTorch tensors, and\n",
    "# Normalize the images with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "for split in [\"train\", \"test\"]:\n",
    "  dataset[split] = CIFAR10(\n",
    "    root=\"data/cifar10\",\n",
    "    train=(split == \"train\"),\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkigVqADNeIN"
   },
   "source": [
    "To train a neural network, we will need to feed data in batches.\n",
    "\n",
    "We create data loaders with the batch size determined previously in setup section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4axnQCtnks_s"
   },
   "outputs": [],
   "source": [
    "dataflow = {}\n",
    "for split in ['train', 'test']:\n",
    "  dataflow[split] = DataLoader(\n",
    "    dataset[split],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=(split == 'train'),\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5G1Lf6hOLGT"
   },
   "source": [
    "We can print the data type and shape from the training data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ReP2g9pD6ppI",
    "outputId": "3ab0f427-ad1d-4c6e-d397-bd1639e7b163"
   },
   "outputs": [],
   "source": [
    "for inputs, targets in dataflow[\"train\"]:\n",
    "  print(f\"[inputs] dtype: {inputs.dtype}, shape: {inputs.shape}\")\n",
    "  print(f\"[targets] dtype: {targets.dtype}, shape: {targets.shape}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPAEVnixjwb7"
   },
   "source": [
    "### Model (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFr1Js3-e3rJ"
   },
   "source": [
    "In this tutorial, we will import MobileNet provided by torchvision, and use the pre-trained weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SNLdS_UQjyBf",
    "outputId": "059a523c-8bdf-443f-b551-897cc1345629"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Load pre-trained MobileNetV2\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "print(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB2X6czdV0px"
   },
   "source": [
    "You should observe that the output dimension of the classifier does not match the number of cleasses in CIFAR-10.\n",
    "\n",
    "Now change the output dimension of the classifer to number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nmo4u51gVzXz",
    "outputId": "c0c3e868-1b53-40c2-c032-622d519aa111"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Change the output dimension of the classifer to number of classes\n",
    "model.classifier[1] = nn.Linear(in_features=model.classifier[1].in_features, out_features=NUM_CLASSES)\n",
    "print(model)\n",
    "\n",
    "# Send the model from cpu to gpu\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEtm9nswWT1z"
   },
   "source": [
    "Now the output dimension of the classifer matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_RcCWoQ8Kp1"
   },
   "source": [
    "As this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zd4Xu-vMyz39"
   },
   "source": [
    "* The model size can be estimated by the number of trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gTfqC0B7Uzi",
    "outputId": "e95058ee-5cfc-4a32-80d9-111c3c0a3bc6"
   },
   "outputs": [],
   "source": [
    "num_params = 0\n",
    "for param in model.parameters():\n",
    "  if param.requires_grad:\n",
    "    num_params += param.numel()\n",
    "print(\"#Params:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZoIKIbzLa4"
   },
   "source": [
    "* The computation cost can be estimated by the number of [multiply–accumulate operations (MACs)](https://en.wikipedia.org/wiki/Multiply–accumulate_operation) using [TorchProfile](https://github.com/zhijian-liu/torchprofile), we will further use this profiling tool in the future labs ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OKVmyWCN7qpp",
    "outputId": "1b15a700-e64b-4893-8288-355e12a36348"
   },
   "outputs": [],
   "source": [
    "num_macs = profile_macs(model, torch.zeros(1, 3, 224, 224).cuda())\n",
    "print(\"#MACs:\", num_macs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYkqpfejzxwq"
   },
   "source": [
    "This model has 2.2M parameters and requires 306M MACs for inference. We will work together in the next few labs to improve its efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjDsY9_KkIjZ"
   },
   "source": [
    "### Optimization (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRg_5KeKLHPj"
   },
   "source": [
    "As we are working on a classification problem, we will apply [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as our loss function to optimize the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K0DEhGKkKfF"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Apply cross entropy as our loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3H8YniYeLIdg"
   },
   "source": [
    "We should decide an optimizer for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXANib83LATH"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Choose an optimizer.\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9X8SiWYLJw2"
   },
   "source": [
    "(Optional) We can apply a learning rate scheduler during the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mJU5aw8KrVX"
   },
   "outputs": [],
   "source": [
    "# TODO(optional):\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2UFRbRYly50"
   },
   "source": [
    "### Training (25%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpHZJpjR7Wy3"
   },
   "source": [
    "We first define the function that optimizes the model for one batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79GKx_oVl09b"
   },
   "outputs": [],
   "source": [
    "def train_one_batch(\n",
    "  model: nn.Module,\n",
    "  criterion: nn.Module,\n",
    "  optimizer: Optimizer,\n",
    "  inputs: torch.Tensor,\n",
    "  targets: torch.Tensor,\n",
    "  scheduler\n",
    ") -> None:\n",
    "\n",
    "    # TODO:\n",
    "    # Step 1: Reset the gradients (from the last iteration)\n",
    "    optimizer.zero_grad()\n",
    "    # Step 2: Forward inference\n",
    "    outputs = model(inputs)\n",
    "    # Step 3: Calculate the loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    # Step 4: Backward propagation\n",
    "    loss.backward()\n",
    "    # Step 5: Update optimizer\n",
    "    optimizer.step()\n",
    "    # (Optional Step 6: scheduler)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3kePTCanalUE"
   },
   "source": [
    "We then define the training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SWx96SGajDR"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    dataflow: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: Optimizer,\n",
    "    scheduler: LRScheduler\n",
    "):\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n",
    "    # Move the data from CPU to GPU\n",
    "    inputs = inputs.cuda()\n",
    "    targets = targets.cuda()\n",
    "\n",
    "    # Call train_one_batch function\n",
    "    train_one_batch(model, criterion, optimizer, inputs, targets, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGaYWFFCbD32"
   },
   "source": [
    "Last, we define the evaluation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXVXHqimbOIo"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "  model: nn.Module,\n",
    "  dataflow: DataLoader\n",
    ") -> float:\n",
    "\n",
    "    model.eval()\n",
    "    num_samples = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n",
    "            # TODO:\n",
    "            # Step 1: Move the data from CPU to GPU\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            # Step 2: Forward inference\n",
    "            outputs = model(inputs)\n",
    "            # Step 3: Convert logits to class indices (predicted class)\n",
    "            predicts = torch.argmax(outputs, dim=1)\n",
    "            # Update metrics\n",
    "            num_samples += targets.size(0)\n",
    "            num_correct += (predicts == targets).sum()\n",
    "\n",
    "    return (num_correct / num_samples * 100).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWCOYuj5cNg3"
   },
   "source": [
    "With training and evaluation functions, we can finally start training the model!\n",
    "\n",
    "If the training is done properly, the accuracy should simply reach higher than 0.925:\n",
    "\n",
    "***Please screenshot the output model accuracy, hand in as YourID_acc_1.png***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "da2105abc42f49f58ca72c5ea950753e",
      "a5fdd6a0756d4e5e8bae6e6da99259e9",
      "21f358813c5349449724e2ede68a015c",
      "4ba547baa4784095a1cae0816072b1a6",
      "09a534f331fe48a387415bdde4af8a1a",
      "bf10f235b9a74e95bfed38f717cdcd48",
      "3e52d9cec3d34532b6b8d62677fa1143",
      "4da6f91bb4204629ba18d89d4f9216dc",
      "d8fa6ab2f7f1470eae2b07ede624c3bf",
      "3f498cd54af24ea88a7feb4ca217f5c4",
      "c88bec1d9e6a4508a766f4ea8e0cd2cb",
      "44a2e6c5293044039b0ae340354c5ed1",
      "d6d9624bda354977bc747878d607ca1d",
      "7383fd9fd76b4771be274c7af5cd18be",
      "66e1f77bc0fd4d608f663266a863f3a6",
      "c9c480b625214769bad67687000cda2e",
      "99783eccda1e409ca59f3181c0a54562",
      "8b53d4bd6fac481695ec757bfa56508d",
      "6c48e367bb74464cbe420c6cc6875d2b",
      "7c51f2b4b6c149c29ff9e3064ec64d5b",
      "1211abbd2733449a95baa97171ec882c",
      "4876ec64a47e45f38eff65845dde138c",
      "c649cc5a698d47f8978be0e4a6339dc2",
      "0e38ed83817543dabfa01d4528224ace",
      "1a55e847be504ed299732f5907b376c5",
      "6ee4d671b1b843928f2fba8ebd4df412",
      "dfe367fe622f47b4a1762a7c99fd0843",
      "198dbf963d4e4d69be61dda0c829379c",
      "32a65171baf249edad7af366e7f7a4ae",
      "d60faaf4237743c6b2f9d2f0ff135889",
      "6f2be5f546874940b7bdea003e5329b8",
      "f78817c819244b5eaa237e2e9ccbc690",
      "1a82d332049e429f89286b12be5f63ca",
      "b9e202c8e7ec45adb504f0d2388bc2df",
      "a0135625e8034f3c924ea8b0d2e79222",
      "d55590165b684c5092239a47b0519419",
      "e323b41574784ce8952ac1ce6a05643e",
      "f4ca82a2bb964d1aababa804e659f682",
      "27a61213438e4c98a36b5b6eec7cdd63",
      "add83ab83cde4bb08da75213be03ec80",
      "032712244b1e4d098e2b19d4f3221477",
      "a3722e131aea4c4d94f66855052c7b90",
      "f3ac1904bad54062982c53b9be2c87d7",
      "09f5c65560324298bd0c9c650d686016",
      "e0fb3fb53b404ad3adc45cf86ec7c9e9",
      "831fe7a4a1b6472ca3ff682d4697b2c9",
      "cdd351bfee0e41bb85767e6a3b92afa0",
      "cc311d8b8b7c405a923126a373023343",
      "577cd8eb74cc42ceacacd967883b965d",
      "7d1b3ad793aa4382a70163e30893c041",
      "c1c9cc4468894bb981de7be959080cc3",
      "4aff6980813e4fd08f20ef9943e1b696",
      "ae79191bc0d3428d968503bab07b13c7",
      "bacd648027824d04b41778e44b09cf43",
      "f93a7262cd4c4d5dab287686be5bb436",
      "ae07b196e5ba49bfa85671f43a291143",
      "050bd0377ef24058afa1ab1ca025405a",
      "c40c1359d1234c72b60aa7ee45c7e95c",
      "f147a7f74ce3416fb00c6f92ea3506b6",
      "2f58d3f3d6324b88b23e09e9a22ffbd6",
      "f2a15eeb2ef842c889cc4ee939a1ac6c",
      "dee47ea2cb6349b298391a9e0cbecee8",
      "b0ce29b11b4c408c89cd16be92d0fcd0",
      "ac33f462bb9b460788a4b35a4f594b9b",
      "d4c4ebbd405a42f8b3e88cec180ac4cb",
      "8d3d4ac1403c44adbc568a17de5840b1",
      "e89b31a1844f427dbc838f96a24e6e39",
      "c3d6705957ed4450855bba3794cdee08",
      "6f99878de77a4dbab0625b2620e01fd8",
      "4cbf35477fbc41c4bb60a76b03c5bafe",
      "b5bda488239642b494548fa24001d08a",
      "7731e220af80458bb1140d765587fab4",
      "06245f65651d49a28ff2eaaaadb9b0f0",
      "1eef4e2d8fc74b19a1b717d632351b30",
      "09ed904a3d4545df8a4303529829cd0a",
      "917849b6f5a4470a9984e9fd457c802e",
      "24f42bf5ba6544898e3fdba7f7a13ced",
      "8b6d833503954f87a974997c3a29d423",
      "a9f5a0c701c84f6aa9cc5c41fece81b9",
      "f3140ec07d334250a4ddb6674171cf5f",
      "05df3a274de447c58d53b264b8538ad5",
      "af177db25cfe4ea99acc5f6284c7073f",
      "6dbd064b36b640e7ad4a866607f26675",
      "07ce3757f1f543de94360da89644fbb8",
      "bcfbf10bc58c4be09c178a55549b121d",
      "84be4fbd2a20447eade0d3741ec9e65e",
      "20e4dd3d74fa4c37aea3018425a3ce29",
      "469cc8dc07d04bcdb634974dd9f24fc0",
      "1c9c2c8f085644d9b6e5286a36812963",
      "dd58ad381a8f445f98b6155a5991b667",
      "63c966ecde794776ae19536a64121cdd",
      "62df09a8e7f34639985d98d9c0d01272",
      "377a01f59ca9492996f9deb410c370a7",
      "e03e1b40b36d43d28ab5661c658c632e",
      "b77117739ab248ca80ac285e7f6af628",
      "ebea39b3a1064a96a90f00b45641bc08",
      "67f5bc67164944a796cc9a9c3e5e45fb",
      "e3a2ea1db36946b58d2d7592d2845883",
      "b3ecdcfd5a784bb48ab026e41b54ea19",
      "f561f40c774f4dd9ad461492df74fc9a",
      "ca0250cc846848d38ad6564f5e569ba8",
      "f3811e659f3345b58907b8c0d8a3aaea",
      "633cabedb798488c8542dd8fa81ca92a",
      "ab339294f4004ae0a8eb5b75b14bf261",
      "fccc1eab56b049d984e7d80220fbbb06",
      "129929cefb754ee0b7246d3d189a10bf",
      "242bd8ae81164714bdae8b80d095e4fd",
      "ee1d6985fb294acda9a13ea4e096eec0",
      "c1c50b9d93064a37aeb7b1bac0f5adf2",
      "21cc60b0018a44fc83318c4825a11b94",
      "286e6c5137ee4d1fb57dc683305b6a5b",
      "dccf46e7f144429cb2fe5f04dde1d785",
      "861ef0cccffc409090a997edfc6533fe",
      "3ff28b9c5cdb426fb14fa98e569cd125",
      "4449acf2411b4472aa519a0248f78586",
      "9c1c5f74d21747e9ac7917e62b66bf45",
      "137edd3bd0a94607966068bb6c4b8b2e",
      "93d1d3b221324fa0b57b3a40fc2db570",
      "5eeb71e67fa94700984f21e26b19292b",
      "7f042c655dfb4bf180faddf58e91d852",
      "515dc59562a5486fa1bf78c1f652444d"
     ]
    },
    "id": "czZObK4OcPD-",
    "outputId": "f1c925e0-01ab-4ef7-8c29-f0fd3a09d0ec"
   },
   "outputs": [],
   "source": [
    "for epoch_num in tqdm(range(1, NUM_EPOCH + 1)):\n",
    "  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n",
    "  acc = evaluate(model, dataflow[\"test\"])\n",
    "  print(f\"epoch {epoch_num}:\", acc)\n",
    "\n",
    "print(f\"final accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao7C2ZSudE0o"
   },
   "source": [
    "Save the weight of the model as \"model.pt\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQrW_B-bcygR"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Save the model weight\n",
    "torch.save(model.state_dict(), \"model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6pbapVjdTG3"
   },
   "source": [
    "You will find \"model.pt\" in the current folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqwrM470RNoO"
   },
   "source": [
    "### Export Model (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHpw1pWF6d1c"
   },
   "source": [
    "We can also save the model weight in [ONNX Format](https://pytorch.org/docs/stable/onnx_torchscript.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yd8oq4-O6kla",
    "outputId": "e4bb8bd1-1030-4131-944c-b8de0968dc4b"
   },
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# TODO:\n",
    "# Specify the input shape\n",
    "dummy_input = torch.randn(1, 3, 224, 224, requires_grad=True).cuda()\n",
    "\n",
    "# TODO:\n",
    "# Export the model to ONNX format\n",
    "onnx_path = 'model.onnx'\n",
    "torch.onnx.export(model, dummy_input, onnx_path, opset_version=11)\n",
    "\n",
    "\n",
    "# TODO:\n",
    "# Export the model to ONNX format\n",
    "\n",
    "print(f\"Model exported to {onnx_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLnLCIuv7M46"
   },
   "source": [
    "In onnx format, we can observe the model structure using [Netron](https://netron.app/).\n",
    "\n",
    "***Please download the model structure, hand in as YourID_onnx.png.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QjgTo0GduJx"
   },
   "source": [
    "### Inference (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEH0zddHdzH-"
   },
   "source": [
    "Load the saved model weight:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i28yt-XOdweL"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Step 1: Get the model structure (mobilenet_v2 and the classifier)\n",
    "loaded_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "loaded_model.classifier[1] = nn.Linear(in_features=loaded_model.classifier[1].in_features, out_features=NUM_CLASSES)\n",
    "\n",
    "# Step 2: Load the model weight from \"model.pt\".\n",
    "model_weight = torch.load(\"model.pt\", weights_only=True)\n",
    "loaded_model.load_state_dict(model_weight)\n",
    "# Step 3: Send the model from cpu to gpu\n",
    "loaded_model = loaded_model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEaZWcuheRFv"
   },
   "source": [
    "Run inference with the loaded model weight and check the accuracy\n",
    "\n",
    "***Please screenshot the output model accuracy, hand in as YourID_acc_2.png***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "referenced_widgets": [
      "dacf631b72c64f7d9bafdbbda0b63c96",
      "3396c6b13a2147529c832f57213b2e1d",
      "01aaacb6bbba4a9fa09fa604145ade9f",
      "0750f7acf2cc465d89f69d7f7e4ef65a",
      "601d45e0a7e94749b73933b50e854407",
      "db3f1c5113014b7b8ef5f9a719e12872",
      "adceb1aa821a4c11b1ba7036b5aae074",
      "9bf9e6634b4a4a23b485e64d02d5d21d",
      "49b93ba4032c41ae887dca2b2845e965",
      "be32868a46594b4f844e0a0b23e25fc1",
      "ee85d11a37844abc9409fb6540107bae"
     ]
    },
    "id": "2dMRsPgGeYe6",
    "outputId": "082383fb-452b-4f55-f9bb-ca8f079ee152"
   },
   "outputs": [],
   "source": [
    "acc = evaluate(loaded_model, dataflow[\"test\"])\n",
    "print(f\"accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytdLy6EIfoeT"
   },
   "source": [
    "If the accurracy is the same as the accuracy before saved, you have completed PART 1.\n",
    "\n",
    "Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ni0FnXXHcgXa"
   },
   "source": [
    "# **Part 2: LLM with torch.compile**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiOzZkitfkmM"
   },
   "source": [
    "In part 2, we will compare the inference speed of the LLM whether we use torch.compile.\n",
    "\n",
    "```torch.compile``` is a new feature in PyTorch 2.0.\n",
    "\n",
    "The following tutorial will help you get to know the usage.\n",
    "\n",
    "[Introduction to torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n",
    "\n",
    "We will choose ```Llama-3.2-1B-Instruct``` as our LLM model.\n",
    "\n",
    "Make sure you have access to llama before starting Part 2.\n",
    "\n",
    "https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bu-ihO_RiaVM"
   },
   "source": [
    "### Loading LLM (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3LSb5aoiodV"
   },
   "source": [
    "We will first install huggingface and login with your token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7EhC4kl0_CUf",
    "outputId": "d259ea8b-c370-42ea-ad0f-cb4ded3b0dc2"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"huggingface_hub[cli]\"\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUD-npfwki3T"
   },
   "source": [
    "We choose LLaMa 3.2 1B Instruct as our LLM model and load the pretrained model.\n",
    "\n",
    "Model ID: **\"meta-llama/Llama-3.2-1B-Instruct\"**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "0f416cae94244b7a8aaaf8909588560c",
      "f6a1644770254d319e04b086a05bd212",
      "84fe6e225b7b4a1f95e9c9cc8d963db0",
      "61fa8c59ae0f47d98d99eb32933c57f0",
      "3396968135ce4fca837ce164dd736640",
      "6369784c83e74f1fb9a3fc684e8ec91f",
      "91fa64b6399b4a548bae67abde4df11d",
      "5fde10b686c1468fa4e322bdccbc859b",
      "472b712df9d544f397627f94050276ab",
      "3c62a025a42f435f86f1f06310d9dcc3",
      "993800c1af4449a1a3370098718600e0",
      "6ef74e12030e41ca8df6b5e82d7d9431",
      "3676ed20ab1a43ea8815e34daf03190b",
      "b6518da7e3314349924fa2d3a1d03a28",
      "86a73b0666fd4af590e482cdee7e2441",
      "a3d99e74b4a747a4a0acd5d1e9689606",
      "746cc4c76c734dfcbbbe0bed738e0537",
      "b1300667ab734835ba41e4a713a150db",
      "c3f033dbcc3f460e8cbf238f2838e16e",
      "4210e4e1523e4e1c967f357d62e948ce",
      "09be8897abe348e68ef76faa92dd53ab",
      "f00af68df3484f36a24531566282a601",
      "c63578dc6e7f47e49082869c0daa70ec",
      "708be900aea44f37b274534c1253c748",
      "cf42a0fc63f6477ea6a30b5820fcf696",
      "a42d355d41554b399d9e8449994bb0cb",
      "0280f8e537ca42169b1476904f0e388b",
      "923fd58de6c6483280243a3f9212737a",
      "641e682785b54192988a4d9a102e370d",
      "86f18d0446ba483fbd28a4f600052370",
      "8ccf3bcb12414d0b84f687fa8e1977d1",
      "3b5be97685fb4f9b9bdef0b67b041e12",
      "c710764290044b498cf1d8b0133491f6",
      "a26a56bf65ba491aa7d09502ac4a33f4",
      "833cef1624af4a3d83ac7c636e2dc923",
      "ead558a20468496591b4916ef973c5ab",
      "eaa19a1e722d4717aaca57faaa91615e",
      "74a148ab725a4d3a8130997093feb8b3",
      "2df899ceced9424c95b18d895214b43e",
      "435a8eccc85241f7bd7dfa3db9b64afc",
      "ee025365cab7472f903d4f2b8a3640ae",
      "7f15bbd454c04c10ad6fee7a5074d17c",
      "1e5863c1b79542b3b2a716f3a7d733c7",
      "202a7be4057f4c239294543460de56ff",
      "140102160d40433197ba0a9b7152ce7f",
      "bcf55ebafeb14860b292811e4bcaaebb",
      "ce567a9316d640138fc66235d1a07f75",
      "9736a2a10caa4c37973dd50071ebaf92",
      "cf56d27b28bf4eba8bf7add042fcda45",
      "cbd2fffc67664cc7989755adcd363f69",
      "c4d67b9aa2c2440281cb05581bc7e822",
      "3f3da1f95dee410bb1f1ae00016c83ff",
      "b5dc2260843646b9a0005c6b9286ccdd",
      "4f0a3538993744dab58542bb1c40f985",
      "b9dcee9af7a549e5974d8346337e2338",
      "932cdcd0c6e447fb86dccd3519ca5475",
      "78fa580a819f4ea99eafa619545dc372",
      "ccbf948328ca4def84865e5e31ba3bd6",
      "590a909701f04b54801847a91773ce09",
      "d545fba9951a42e0857b421e0b7726d6",
      "bfd51228789447eba1fa9fe3f42944c0",
      "e3b850b8b7cd4a84938d4c9f0230daac",
      "f1276bb84b9a4c26b37b2e30bd27412d",
      "42f965e680824719b215a97d8dbdf95e",
      "c576820edd964e5fad08e4470103e52b",
      "da93f637b97d49a3b0adc03921bd28e4"
     ]
    },
    "id": "SjCkbW_i8VUq",
    "outputId": "e88056b2-289e-4d16-e83f-56f2c9cfa7d1"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# TODO:\n",
    "# Load the LLaMA 3.2 1B Instruct model\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1F8VBtODHX2"
   },
   "source": [
    "First we need to decide our prompt to feed into LLM and the maximum token length as well.\n",
    "\n",
    "You can also change the iteration times of testing for the following tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJMPtSLuCzCq"
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Input prompt\n",
    "# You can change the prompt whatever you want, e.g. \"How to learn a new language?\", \"What is Edge AI?\"\n",
    "\n",
    "prompt = \"My name is Tony, and Chloe is my wife\"\n",
    "inputs = tokenizer(model_id, return_tensors=\"pt\").to(\"cuda\")\n",
    "max_token_length = 500\n",
    "iter_times = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiHloHdRKaaY"
   },
   "source": [
    "### Inference with torch.compile (10%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWj3sD_PJL7Z"
   },
   "source": [
    "Let's define a timer function to compare the speed up of ```torch.compile```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNK4ukXZb631"
   },
   "outputs": [],
   "source": [
    "def timed(fn):\n",
    "  start = torch.cuda.Event(enable_timing=True)\n",
    "  end = torch.cuda.Event(enable_timing=True)\n",
    "  start.record()\n",
    "  result = fn()\n",
    "  end.record()\n",
    "  torch.cuda.synchronize()\n",
    "  return result, start.elapsed_time(end) / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4IDTZYbJxJj"
   },
   "source": [
    "After everything is set up, let's start!\n",
    "\n",
    "We first simply run the inference without ```torch.compile```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-u-fBCU-b7hS",
    "outputId": "b2e6d426-eb1d-49b7-edf4-d29f6e60f8ee"
   },
   "outputs": [],
   "source": [
    "original_times = []\n",
    "\n",
    "# Timing without torch.compile\n",
    "for i in range(iter_times):\n",
    "  with torch.no_grad():\n",
    "    original_output, original_time = timed(lambda: model.generate(**inputs, max_length=max_token_length, pad_token_id=tokenizer.eos_token_id))\n",
    "  original_times.append(original_time)\n",
    "  print(f\"Time taken without torch.compile: {original_time} seconds\")\n",
    "\n",
    "# Decode the output\n",
    "output_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
    "print(f\"Output without torch.compile: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeVhUlk_K9La"
   },
   "source": [
    "Before using ```torch.compile```, we need to access the model's ```generation_config``` attribute and set the ```cache_implementation``` to \"static\".\n",
    "\n",
    "To use ```torch.compile```, we need to call ```torch.compile``` on the model to compile the forward pass with the static kv-cache.\n",
    "\n",
    "Reference: https://huggingface.co/docs/transformers/llm_optims?static-kv=basic+usage%3A+generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQkTIDusb_7i",
    "outputId": "89cf2e4f-deb7-409b-a25e-56dcc0c27d0b"
   },
   "outputs": [],
   "source": [
    "compile_times = []\n",
    "\n",
    "# Remind that whenever you use torch.compile, you need to use torch._dynamo.reset() to clear all compilation caches and restores the system to its initial state.\n",
    "import torch._dynamo\n",
    "torch._dynamo.reset()\n",
    "\n",
    "# TODO:\n",
    "# Compile the model\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "compiled_model = torch.compile(model, mode=\"reduce-overhead\", fullgraph=True)\n",
    "\n",
    "# Timing with torch.compile\n",
    "for i in range(iter_times):\n",
    "  with torch.no_grad():\n",
    "    compile_output, compile_time = timed(lambda: compiled_model.generate(**inputs, max_length=max_token_length, pad_token_id=tokenizer.eos_token_id))\n",
    "  compile_times.append(compile_time)\n",
    "  print(f\"Time taken with torch.compile: {compile_time} seconds\")\n",
    "\n",
    "# Decode output\n",
    "output_text = tokenizer.decode(compile_output[0], skip_special_tokens=True)\n",
    "print(f\"\\nOutput with torch.compile: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFZMBADDTo5T"
   },
   "source": [
    "We can easily observe that after the first inference, the inference time drops a lot!\n",
    "\n",
    "Below code can tell you how much faster did ```torch.compile``` did.\n",
    "\n",
    "***Please screenshot the inference time and speedup below, hand in as YourID_speedup.png***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPbqAgo6P7et",
    "outputId": "0e3c7983-ba4e-45c6-a7df-7e03b3564c27"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "original_med = np.median(original_times)\n",
    "compile_med = np.median(compile_times)\n",
    "speedup = original_med / compile_med\n",
    "print(f\"Original median: {original_med},\\nCompile median: {compile_med},\\nSpeedup: {speedup}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXdb901VmrZS"
   },
   "source": [
    "You've finished part 2.\n",
    "\n",
    "Congratulations!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
