{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e27b0a6",
   "metadata": {},
   "source": [
    "## Part 2: Matrix Multiplication in Triton(40%)\n",
    "\n",
    "In this part, you will write a very short high-performance FP16 matrix multiplication kernel.\n",
    "\n",
    "Problem 9 and 10 **do** need to run on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a415a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "from torch import Tensor\n",
    "import triton.language as tl\n",
    "import jaxtyping\n",
    "from jaxtyping import Float32, Int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87432d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Setup\n",
    "\n",
    "import triton_viz\n",
    "import inspect\n",
    "from triton_viz.interpreter import record_builder\n",
    "\n",
    "def test(puzzle, puzzle_spec, nelem={}, B={\"B0\": 32}, viz=True):\n",
    "    B = dict(B)\n",
    "    if \"N1\" in nelem and \"B1\" not in B:\n",
    "        B[\"B1\"] = 32\n",
    "    if \"N2\" in nelem and \"B2\" not in B:\n",
    "        B[\"B2\"] = 32\n",
    "\n",
    "    triton_viz.interpreter.record_builder.reset()\n",
    "    torch.manual_seed(0)\n",
    "    signature = inspect.signature(puzzle_spec)\n",
    "    args = {}\n",
    "    for n, p in signature.parameters.items():\n",
    "        print(p)\n",
    "        args[n + \"_ptr\"] = ([d.size for d in p.annotation.dims], p)\n",
    "    args[\"z_ptr\"] = ([d.size for d in signature.return_annotation.dims], None)\n",
    "\n",
    "    tt_args = []\n",
    "    for k, (v, t) in args.items():\n",
    "        tt_args.append(torch.rand(*v) - 0.5)\n",
    "        if t is not None and t.annotation.dtypes[0] == \"int32\":\n",
    "            tt_args[-1] = torch.randint(-100000, 100000, v)\n",
    "    grid = lambda meta: (triton.cdiv(nelem[\"N0\"], meta[\"B0\"]),\n",
    "                         triton.cdiv(nelem.get(\"N1\", 1), meta.get(\"B1\", 1)),\n",
    "                         triton.cdiv(nelem.get(\"N2\", 1), meta.get(\"B2\", 1)))\n",
    "\n",
    "    #for k, v in args.items():\n",
    "    #    print(k, v)\n",
    "    triton_viz.trace(puzzle)[grid](*tt_args, **B, **nelem)\n",
    "    z = tt_args[-1]\n",
    "    tt_args = tt_args[:-1]\n",
    "    z_ = puzzle_spec(*tt_args)\n",
    "    match = torch.allclose(z, z_, rtol=1e-3, atol=1e-3)\n",
    "    print(\"Results match:\",  match)\n",
    "    failures = False\n",
    "    if viz:\n",
    "        failures = triton_viz.launch()\n",
    "    if not match or failures:\n",
    "        print(\"Invalid Access:\", failures)\n",
    "        print(\"Yours:\", z)\n",
    "        print(\"Spec:\", z_)\n",
    "        print(torch.isclose(z, z_))\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5b743",
   "metadata": {},
   "source": [
    "## Question 9 (10 pts): Matrix Mult\n",
    "\n",
    "Now, let's implement a naive matmul in Triton.\n",
    "Because we frequently create 1d- or 2d-offets and -masks, let's put that functionality into utility functions. As long as these functions are triton.jit-ed, they can be used in the kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489df5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def get_1d_offest(size, n_prev_block):\n",
    "    return #YOUR CODE HERE, ONE LINE ONLY\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_offset(offs_0, offs_1, stride_0, stride_1=1):\n",
    "    return #YOUR CODE HERE, ONE LINE ONLY\n",
    "\n",
    "@triton.jit\n",
    "def get_1d_mask(offs, max):\n",
    "    return offs < max\n",
    "\n",
    "@triton.jit\n",
    "def get_2d_mask(offs_0, offs_1, max_0, max_1):\n",
    "    return #YOUR CODE HERE, ONE LINE ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f220044",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def naive_matmul_kernel(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    m, n, k,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # get 1d offsets\n",
    "    rm = \n",
    "    rn = \n",
    "    rk = \n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # relevant offsets of a, b\n",
    "    offs_a = \n",
    "    offs_b = \n",
    "\n",
    "\n",
    "    # initialize and iteratively update accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for _ in range(0, k, BLOCK_SIZE_K):\n",
    "        a = tl.load(offs_a)\n",
    "        b = tl.load(offs_b)\n",
    "        acc += tl.dot(a, b, allow_tf32=False) # matmul in block\n",
    "        # YOUR CODE HERE\n",
    "        # increase offets, so next iteration loads next block\n",
    "       \n",
    "\n",
    "    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n",
    "    mask = get_2d_mask(rm, rn, m, n)\n",
    "    tl.store(c, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb19c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matmul(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    BLOCK_SIZE_M = 32\n",
    "    BLOCK_SIZE_N = 32\n",
    "    BLOCK_SIZE_K = 32\n",
    "    grid = lambda META: (triton.cdiv(M, BLOCK_SIZE_M) , triton.cdiv(N, BLOCK_SIZE_N) )\n",
    "    naive_matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "        BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N,\n",
    "        BLOCK_SIZE_K,\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04085d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "triton_output = naive_matmul(a, b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68ec065",
   "metadata": {},
   "source": [
    "## Question 10 (20 pts): Faster Matrix Mult\n",
    "\n",
    "Note: Needs code from example 3, so run that before\n",
    "\n",
    "Triton handles the order of memory access **within** blocks, but not **across** blocks. So this is a knob we can use to make our kernels faster.\n",
    "\n",
    "In fact, cleverly reordering blocks can increase L2-cache hit rate, which makes our kernels faster. This example is taken from the [triton docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html).\n",
    "\n",
    "Now, to make better use of the L2 cache, we want to reuse data that's was recently loaded, and is therefore likely still in the L2 cache. How? By reducing the number of _different_ data loads that a bunch of \"consecutive\" kernels need. By \"consecutive\" we mean kernels that are executed approximately at the same time.\n",
    "\n",
    "This picture (adapter from the [triton docs](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html)) shows how we can do that. If we order naively, the first row of the output matrix is computed \"consecutively\", which needs 90 different block reads (9 from matrix A, 81 from matrix B). If we use \"group ordering\", a 3x3 square of blocks of the output matrix is computed \"consecutively\", which needs 54 different block reads (27 from matrix A, 27 from matrix B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83516ea",
   "metadata": {},
   "source": [
    "<img src='https://triton-lang.org/main/_images/grouped_vs_row_major_ordering.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15e114",
   "metadata": {},
   "source": [
    "The default ordering is shown (called \"row-major\"). Remember, we deal with blocks. We can't arrange how the individual cells are processed, only the blocks. In the picture, our output matrix C has 5x7 = 35 cells, but only cdiv(5,1) x cdiv(7,2) = 5x4 = 20 blocks.\n",
    "\n",
    "When perform grouped ordering, notice how the first 9 processed blocks are the 3x3 grid we want! We process 3 blocks in a column. Then advance a column, again process 3, advance, and so on. The orange lines show where advance. This operation is called \"swizzling\".\n",
    "\n",
    "By the way, you can of course change the number 3. It's called the group_size.\n",
    "\n",
    "Triton provide a triton.language.swizzle2d function.\n",
    "\n",
    "To really understand swizzle2d, try to implement your own swizzle2d function ,and quickly verifiy it works as expected. We'll then continue to use it in our faster matmul kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7449952",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def swizzle(x_ptr, z_ptr, group_size: tl.constexpr):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "    pid_m_, pid_n_ = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_size)  # tl.swizzle2d doesn't work when simulating on CPU\n",
    "\n",
    "    offs_m = get_1d_offest(1, n_prev_block=pid_m)\n",
    "    offs_n = get_1d_offest(1, n_prev_block=pid_n)\n",
    "\n",
    "    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)\n",
    "    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n)\n",
    "\n",
    "    offs_sw_m = get_1d_offest(1, n_prev_block=pid_m_)\n",
    "    offs_sw_n = get_1d_offest(1, n_prev_block=pid_n_)\n",
    "\n",
    "    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)\n",
    "    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)\n",
    "\n",
    "    x = tl.load(x_ptr + offs, mask=mask)\n",
    "    tl.store(z_ptr + offs_sw, x, mask=mask_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a97d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def your_swizzle(x_ptr, z_ptr, group_size: tl.constexpr):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    ######################\n",
    "\n",
    "    pid_m_, pid_n_ = \n",
    "\n",
    "    ######################\n",
    "\n",
    "    offs_m = get_1d_offest(1, n_prev_block=pid_m)\n",
    "    offs_n = get_1d_offest(1, n_prev_block=pid_n)\n",
    "\n",
    "    offs = get_2d_offset(offs_m, offs_n, stride_0=num_pid_n)\n",
    "    mask = get_2d_mask(offs_m, offs_n, max_0=num_pid_m, max_1=num_pid_n )\n",
    "\n",
    "    offs_sw_m = get_1d_offest(1, n_prev_block=pid_m_)\n",
    "    offs_sw_n = get_1d_offest(1, n_prev_block=pid_n_)\n",
    "\n",
    "    offs_sw = get_2d_offset(offs_sw_m, offs_sw_n, stride_0=num_pid_n)\n",
    "    mask_sw = get_2d_mask(offs_sw_m, offs_sw_n, max_0=num_pid_m, max_1=num_pid_n)\n",
    "\n",
    "    x = tl.load(x_ptr + offs, mask=mask)\n",
    "    tl.store(z_ptr + offs_sw, x, mask=mask_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be5c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks_m, blocks_n = 5,4\n",
    "\n",
    "x = torch.arange(blocks_m*blocks_n, device='cuda').view(blocks_m,blocks_n)\n",
    "z1 = -torch.ones_like(x) # empty matrix, with -1 denoting empty\n",
    "z2 = -torch.ones_like(x) # empty matrix, with -1 denoting empty\n",
    "\n",
    "swizzle[(blocks_m, blocks_n)](x, z1, group_size=3)\n",
    "your_swizzle[(blocks_m,blocks_n)](x,z2, group_size=3)\n",
    "print(z1)\n",
    "print(z2)\n",
    "if torch.allclose(z1, z2, atol=0, rtol=0):\n",
    "    print(\"✅ match\")\n",
    "else:\n",
    "    print(\"❌ not match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998d8a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def grouped_matmul_kernel(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    m, n, k,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K : tl.constexpr, group_size: tl.constexpr\n",
    "):\n",
    "    pid_m, pid_n = tl.program_id(0), tl.program_id(1)\n",
    "    num_pid_m, num_pid_n = tl.num_programs(0), tl.num_programs(1)\n",
    "    # determine location of block in grouped ordering - swizzle!\n",
    "    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, group_size)  # tl.swizzle2d doesn't work when simulating on CPU\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # get 1d offsets\n",
    "    rm = \n",
    "    rn = \n",
    "    rk = \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    # relevant offsets of a, b\n",
    "    offs_a = \n",
    "    offs_b = \n",
    "\n",
    "    # initialize and iteratively update accumulator\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "\n",
    "    for _ in range(0, k, BLOCK_SIZE_K):\n",
    "        a = tl.load(offs_a)\n",
    "        b = tl.load(offs_b)\n",
    "        acc += tl.dot(a, b, allow_tf32=False) # block level matrix multiplication\n",
    "\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # increase offets, so next iteration loads next chunks\n",
    "\n",
    "\n",
    "    c = c_ptr + get_2d_offset(rm, rn, stride_cm, stride_cn)\n",
    "    mask = get_2d_mask(rm, rn, m, n)\n",
    "    tl.store(c, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402f2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_matmul(a, b, group_size):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    BLOCK_SIZE_M = 64\n",
    "    BLOCK_SIZE_N = 64\n",
    "    BLOCK_SIZE_K = 64\n",
    "    grid = lambda META: (triton.cdiv(M, BLOCK_SIZE_M) ,triton.cdiv(N, BLOCK_SIZE_N) )\n",
    "    grouped_matmul_kernel[grid](\n",
    "        a, b, c,  #\n",
    "        M, N, K,  #\n",
    "        a.stride(0), a.stride(1),  #\n",
    "        b.stride(0), b.stride(1),  #\n",
    "        c.stride(0), c.stride(1),  #\n",
    "        BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N,\n",
    "        BLOCK_SIZE_K,\n",
    "        group_size  #\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171cf133",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "a = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "b = torch.randn((512, 512), device='cuda', dtype=torch.float16)\n",
    "triton_output = grouped_matmul(a, b, group_size=32)\n",
    "torch_output = torch.matmul(a, b)\n",
    "if torch.allclose(triton_output, torch_output, atol=5e-2, rtol=0):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff423150",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "##Benchmarking##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd54f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "We can now compare the performance of our kernel. Here we focus on square matrices, but feel free to arrange this script as you wish to benchmark any other matrix shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32262dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=['square_matrix_size'],  # Argument names to use as an x-axis for the plot.\n",
    "        x_vals=[2**i for i in range(5, 12, 1)],  # Different possible values for `x_name`.\n",
    "        x_log=True,  # x axis is logarithmic.\n",
    "        line_arg='provider',  # Argument name whose value corresponds to a different line in the plot.\n",
    "        line_vals=['naive', 'grouped', 'torch'],  # Possible values for `line_arg`.\n",
    "        line_names=['Naive', 'Grouped', 'Torch'],  # Label name for the lines.\n",
    "        styles=[('blue', '-'), ('green', '-'), ('orange','-')],  # Line styles.\n",
    "        ylabel='GB/s',  # Label name for the y-axis.\n",
    "        plot_name='matmul-performance',  # Name for the plot. Used also as a file name for saving the plot.\n",
    "        args={},  # Values for function arguments not in `x_names` and `y_name`.\n",
    "    ))\n",
    "def benchmark(square_matrix_size, provider):\n",
    "    sz = square_matrix_size\n",
    "    a = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n",
    "    b = torch.rand((sz, sz), device='cuda', dtype=torch.float32)\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == 'naive':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: naive_matmul(a, b), quantiles=quantiles)\n",
    "    if provider == 'grouped': ms, min_ms, max_ms = triton.testing.do_bench(lambda: grouped_matmul(a, b, group_size=8), quantiles=quantiles)\n",
    "    if provider == 'torch':   ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a,b), quantiles=quantiles)\n",
    "    gbps = lambda ms: 12 * sz / ms * 1e-6\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.run(print_data=True, show_plots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
